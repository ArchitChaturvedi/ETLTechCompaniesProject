
# ETL Pipeline for Tech Companies Dataset

This Github marks a data engineering project that demonstrates the end-to-end ETL process using modern data engineering tools, including **Apache Airflow**, **PostgreSQL**, and **Docker**. The pipeline extracts data from CSV files, loads it into a PostgreSQL database hosted on **Neon**, and orchestrates the workflow using Airflow.

---

## Project Overview

This project simulates a real-world ETL pipeline for ingesting and managing data regarding the top tech companies from three different sectors, as well as overall tech companies, based on total revenue from 2024:

- Consumer Electronics
- Cybersecurity
- Semiconductors
- General Tech Companies

The goal here is to showcase data ingestion, transformation, and orchestration using production-grade tools, as well as to demonstrate skills in ETL Pipeline development and data engineering.

---

## Tools/Skills Used

- **Python** – scripting and ETL Pipeline development
- **Python** – ETL scripting - main version used in project
- **Airflow** – workflow orchestration and DAGs
- **PostgreSQL (Neon)** – cloud-hosted relational database
- **Docker** – containerization of services
- **SQL** – data modeling and querying
- **SQL** – ETL scripting (alternative version to demonstrate Spark skills)
---

## Project Structure:
<img width="922" height="577" alt="image" src="https://github.com/user-attachments/assets/5d0288dc-64fa-4d40-acea-1456dfd78151" />

